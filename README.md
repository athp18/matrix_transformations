Implementing machine learning models with PyTorch tensors, without most of its abstractions, such as torch.nn, torch.optim, or autograd. This is basically how would I do it in NumPy, but uses PyTorch's built-in GPU support for a faster training process.
